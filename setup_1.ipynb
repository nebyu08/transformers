{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f64c862-4d7f-45b0-b04e-d63e270a0917",
   "metadata": {},
   "source": [
    "# this is for handling the dataset of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d737461a-7acc-4b91-a687-f07c843a5fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=\"data\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "file_path=os.path.join(directory,\"merged_file.txt\")\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with open(\"C:/Users/nebiy/OneDrive/Documents/personal_project_datasets/BattleCreekDec19_2019.txt\",\"r\") as file1,open(\"C:/Users/nebiy/OneDrive/Documents/personal_project_datasets/BemidjiSep18_2020.txt\",\"r\") as file2,open(\"C:/Users/nebiy/OneDrive/Documents/personal_project_datasets/CharlotteMar2_2020.txt\",\"r\") as file3:\n",
    "            content1=file1.read()\n",
    "            content2=file2.read()\n",
    "            content3=file3.read()\n",
    "    \n",
    "    #closing the files i opened for reading\n",
    "    file1.close()\n",
    "    file2.close()\n",
    "    file3.close()\n",
    "    with open(\"data/merged_file.txt\",\"w\") as merged_file:\n",
    "        #merge the files\n",
    "        merged_file.write(content1+content2+content3)\n",
    "else:\n",
    "    print(\"the file already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d6dee1-8f81-4748-a6cc-9f377b1d79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets import the dataset\n",
    "with open(file_path,\"r\") as file:\n",
    "    all_text=file.read()\n",
    "    non_text=re.compile(r'[^a-zA-Z\\s]')\n",
    "    text=non_text.sub(\"\",all_text)\n",
    "    \n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5aa25d3-dd8f-43f9-aa52-8687825b4eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '23', ' wor', \"'fe\", ' never', '343', '3', \"'\", '9', 'k']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "gpt_tokenizer=r\"\"\"(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\"\"\n",
    "sentence=\"hello23 wor'fe never3433'9k\"\n",
    "print(re.findall(gpt_tokenizer,sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "381c81ef-9c52-43a5-8c75-958fc91f18a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pattern_object=re.compile(gpt_tokenizer)\n",
    "temp=list(re.findall(pattern_object,sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48143dc2-dbf7-4953-a022-882ed2b0ad54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'hello', b'23', b' wor', b\"'fe\", b' never', b'343', b'3', b\"'\", b'9', b'k']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.encode(\"utf-8\") for i in temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "979127be-af37-4ca3-b5f1-40c0be173882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from base import BaseTokenizer\n",
    "from setup import merge,get_stats\n",
    "\n",
    "gpt2_regexpression=r\"\"\"(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "gpt4_regexpression=r\"\"\"(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "class RegexTokenizer(BaseTokenizer):\n",
    "    def __inti__(self,pattern):\n",
    "        super().__init__()\n",
    "        self.merges={}\n",
    "        self.special_tokens={}\n",
    "        self.pattern=gpt4_regexpression if pattern is None else pattern\n",
    "        self.pattern_object=re.compile(self.pattern)\n",
    "\n",
    "    def train(self,text,vocab_size,versbose=False):\n",
    "        assert vocab_size>=256,\"make a good an appropriate vocab size\"\n",
    "        num_embedding=vocab_size-256\n",
    "        print(f\"this is num_embeddding {num_embedding}\")\n",
    "        chunks=list(re.findall(self.pattern,text))\n",
    "        print(f\"chunk is {chunks} \")\n",
    "        chunks_encode=[i.encode(\"utf-8\") for i in chunks]\n",
    "        print(f\"chunk encoded is {chunks_encode}\")\n",
    "        stats={}\n",
    "        #find the pairs counts across the chunks\n",
    "        for chunk in chunks:\n",
    "            get_stats(chunks_encode,stats)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8658094b-9b20-44a1-8725-44c7c574c64d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
